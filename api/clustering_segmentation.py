"""
Clustering & Segmentation API for FreshRetailNet-50K
Groups similar products, stores, and cities based on:
- Demand profiles
- Stockout rates  
- Discount responsiveness
- Weather sensitivity
"""

import asyncio
import asyncpg
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Any, Tuple
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import logging
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import json
import warnings

warnings.filterwarnings("ignore")

from database.connection import DatabaseManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()
db_manager = DatabaseManager()


# Pydantic models
class ClusteringRequest(BaseModel):
    entity_type: str  # 'products', 'stores', 'cities'
    clustering_algorithm: str = "kmeans"  # 'kmeans', 'dbscan'
    n_clusters: Optional[int] = None
    features: List[str] = [
        "demand_profile",
        "stockout_rate",
        "discount_response",
        "weather_sensitivity",
    ]
    min_data_points: int = 7  # Reduced for testing
    analysis_period_days: int = 90  # Reduced to match available data range


class ClusterProfile(BaseModel):
    cluster_id: int
    cluster_name: str
    entity_count: int
    characteristics: Dict[str, Any]
    key_metrics: Dict[str, float]
    representative_entities: List[Dict[str, Any]]


class EntityClusterInfo(BaseModel):
    entity_id: str
    entity_name: str
    cluster_id: int
    cluster_name: str
    similarity_score: float
    key_attributes: Dict[str, Any]


class ClusteringResponse(BaseModel):
    success: bool
    entity_type: str
    algorithm_used: str
    n_clusters: int
    cluster_profiles: List[ClusterProfile]
    entity_assignments: List[EntityClusterInfo]
    cluster_quality_metrics: Dict[str, float]
    insights: List[str]
    recommendations: List[Dict[str, Any]]


@router.post("/cluster-analysis")
async def perform_clustering_analysis(request: ClusteringRequest):
    """
    Perform clustering analysis on products, stores, or cities
    """
    try:
        await db_manager.initialize()

        logger.info(f"Starting clustering analysis for {request.entity_type}")

        # Get connection
        async with db_manager.get_connection() as conn:
            # Extract features for clustering
            features_df = await extract_clustering_features(
                conn,
                request.entity_type,
                request.features,
                request.analysis_period_days,
                request.min_data_points,
            )

            logger.info(
                f"Features DataFrame generated by extract_clustering_features: {len(features_df)} rows, {features_df.columns.tolist()} columns"
            )
            if features_df.empty:
                return {
                    "success": False,
                    "detail": "Insufficient data for clustering analysis",
                }

            logger.info(f"Extracted {len(features_df)} entities for clustering")

            # Perform clustering
            clustering_result = await perform_clustering(
                features_df, request.clustering_algorithm, request.n_clusters
            )

            # Generate cluster profiles, entity assignments, and quality metrics
            cluster_profiles = await generate_cluster_profiles(
                conn, clustering_result, request.entity_type, request.features
            )
            entity_assignments = await generate_entity_assignments(
                conn, clustering_result, request.entity_type
            )
            cluster_quality_metrics = calculate_cluster_quality_metrics(
                clustering_result
            )
            insights = generate_clustering_insights(
                cluster_profiles, request.entity_type
            )
            recommendations = generate_clustering_recommendations(
                cluster_profiles, request.entity_type
            )

            return ClusteringResponse(
                success=True,
                entity_type=request.entity_type,
                algorithm_used=request.clustering_algorithm,
                n_clusters=clustering_result["n_clusters"],
                cluster_profiles=cluster_profiles,
                entity_assignments=entity_assignments,
                cluster_quality_metrics=cluster_quality_metrics,
                insights=insights,
                recommendations=recommendations,
            )

    except Exception as e:
        logger.error(f"Error in clustering analysis: {e}")
        return {"success": False, "detail": str(e)}


async def extract_clustering_features(
    conn: asyncpg.Connection,
    entity_type: str,
    features: List[str],
    analysis_period_days: int,
    min_data_points: int,
) -> pd.DataFrame:
    """
    Extract features for clustering analysis
    """
    try:
        # Dynamically determine start and end dates from sales_data table
        date_range_query = """
        SELECT MIN(dt) as min_date, MAX(dt) as max_date
        FROM sales_data
        """
        date_range_result = await conn.fetchrow(date_range_query)

        if (
            not date_range_result
            or not date_range_result["min_date"]
            or not date_range_result["max_date"]
        ):
            logger.warning("No date range found in sales_data table.")
            return pd.DataFrame()

        db_min_date = date_range_result["min_date"]
        db_max_date = date_range_result["max_date"]

        # Ensure dates are datetime.date objects
        if isinstance(db_min_date, str):
            db_min_date = datetime.strptime(db_min_date, "%Y-%m-%d").date()
        if isinstance(db_max_date, str):
            db_max_date = datetime.strptime(db_max_date, "%Y-%m-%d").date()

        # Use a window of analysis_period_days ending at the max_date in the database
        end_date = db_max_date
        start_date = db_max_date - timedelta(days=analysis_period_days)

        # Ensure start_date is not before the actual min_date in the database
        if start_date < db_min_date:
            start_date = db_min_date

        logger.info(
            f"Using analysis period from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}"
        )

        if entity_type == "products":
            df = await extract_product_features(
                conn, features, start_date, end_date, min_data_points
            )
        elif entity_type == "stores":
            df = await extract_store_features(
                conn, features, start_date, end_date, min_data_points
            )
        elif entity_type == "cities":
            df = await extract_city_features(
                conn, features, start_date, end_date, min_data_points
            )
        else:
            raise ValueError(f"Unsupported entity type: {entity_type}")

        logger.info(
            f"DataFrame returned from {entity_type} feature extraction: {len(df)} rows, {df.columns.tolist()} columns"
        )
        return df

    except Exception as e:
        logger.error(f"Error extracting clustering features: {e}")
        raise


async def extract_product_features(
    conn: asyncpg.Connection,
    features: List[str],
    start_date: date,
    end_date: date,
    min_data_points: int,
) -> pd.DataFrame:
    """
    Extract product-level features for clustering (real data)
    """
    try:
        query = """
        WITH product_metrics AS (
            SELECT 
                p.product_id,
                p.product_name,
                p.first_category_id,
                p.second_category_id,
                p.third_category_id,
                COUNT(DISTINCT s.dt) as active_days,
                COUNT(*) as total_transactions,
                AVG(CAST(s.sale_amount AS FLOAT)) as avg_sale_amount,
                STDDEV(CAST(s.sale_amount AS FLOAT)) as sale_amount_stddev,
                AVG(CASE WHEN (s.discount > 0) IS TRUE THEN s.discount ELSE NULL END) as avg_discount,
                COUNT(CASE WHEN (s.discount > 0) IS TRUE THEN 1 END)::FLOAT / COUNT(*) as discount_frequency,
                AVG(CASE WHEN s.activity_flag = '1' THEN CAST(s.sale_amount AS FLOAT) ELSE NULL END) as avg_promo_sale,
                AVG(CASE WHEN s.activity_flag = '0' THEN CAST(s.sale_amount AS FLOAT) ELSE NULL END) as avg_regular_sale,
                AVG(CASE WHEN s.holiday_flag = '1' THEN CAST(s.sale_amount AS FLOAT) ELSE NULL END) as avg_holiday_sale,
                COUNT(CASE WHEN s.holiday_flag = '1' THEN 1 END)::FLOAT / COUNT(*) as holiday_sales_ratio
            FROM product_hierarchy p
            JOIN sales_data s ON p.product_id = s.product_id
            WHERE s.dt BETWEEN $1 AND $2
            GROUP BY p.product_id, p.product_name, p.first_category_id, p.second_category_id, p.third_category_id
            -- Diagnostic: Log counts before filtering
            -- SELECT p.product_id, COUNT(DISTINCT s.dt) as distinct_active_days FROM product_hierarchy p JOIN sales_data s ON p.product_id = s.product_id WHERE s.dt BETWEEN $1 AND $2 GROUP BY p.product_id
            HAVING COUNT(DISTINCT s.dt) >= $3
        ),
        demand_variability AS (
            SELECT 
                product_id,
                STDDEV(daily_sales) / NULLIF(AVG(daily_sales), 0) as demand_cv,
                COUNT(CASE WHEN daily_sales = 0 THEN 1 END)::FLOAT / COUNT(*) as zero_sales_ratio
            FROM (
                SELECT 
                    product_id,
                    dt,
                    SUM(CAST(sale_amount AS FLOAT)) as daily_sales
                FROM sales_data
                WHERE dt BETWEEN $1 AND $2
                GROUP BY product_id, dt
            ) daily_agg
            GROUP BY product_id
        ),
        stockout_analysis AS (
            SELECT 
                product_id,
                AVG(CASE WHEN stock_hour6_22_cnt = '0' THEN 1.0 ELSE 0.0 END) as stockout_rate,
                AVG(CAST(stock_hour6_22_cnt AS FLOAT)) as avg_stock_level
            FROM sales_data
            WHERE dt BETWEEN $1 AND $2
            GROUP BY product_id
        ),
        weather_sensitivity AS (
            SELECT 
                s.product_id,
                CORR(CAST(s.sale_amount AS FLOAT), s.avg_temperature) as temp_correlation,
                CORR(CAST(s.sale_amount AS FLOAT), s.precpt) as precip_correlation,
                CORR(CAST(s.sale_amount AS FLOAT), s.avg_humidity) as humidity_correlation
            FROM sales_data s
            WHERE s.dt BETWEEN $1 AND $2
            GROUP BY s.product_id
            HAVING COUNT(*) >= $3
        )
        SELECT 
            pm.*,
            COALESCE(dv.demand_cv, 0) as demand_coefficient_variation,
            COALESCE(dv.zero_sales_ratio, 0) as zero_sales_ratio,
            COALESCE(sa.stockout_rate, 0) as stockout_rate,
            COALESCE(sa.avg_stock_level, 0) as avg_stock_level,
            COALESCE(ws.temp_correlation, 0) as temperature_sensitivity,
            COALESCE(ws.precip_correlation, 0) as precipitation_sensitivity,
            COALESCE(ws.humidity_correlation, 0) as humidity_sensitivity,
            CASE 
                WHEN pm.avg_promo_sale IS NOT NULL AND pm.avg_regular_sale IS NOT NULL 
                THEN (pm.avg_promo_sale - pm.avg_regular_sale) / NULLIF(pm.avg_regular_sale, 0)
                ELSE 0 
            END as discount_response_ratio
        FROM product_metrics pm
        LEFT JOIN demand_variability dv ON pm.product_id = dv.product_id
        LEFT JOIN stockout_analysis sa ON pm.product_id = sa.product_id
        LEFT JOIN weather_sensitivity ws ON pm.product_id = ws.product_id
        ORDER BY pm.product_id
        """
        rows = await conn.fetch(
            query,
            start_date.strftime("%Y-%m-%d"),
            end_date.strftime("%Y-%m-%d"),
            min_data_points,
        )

        if rows:
            logger.debug(f"First row keys from asyncpg: {rows[0].keys()}")

        df = pd.DataFrame(rows, columns=rows[0].keys() if rows else [])
        logger.info(f"Product features query returned {len(df)} rows")

        if df.empty:
            logger.warning("No product features found for clustering query.")
            logger.info("DataFrame is empty, returning empty dataframe")
            return pd.DataFrame()

        # Select and prepare features based on request
        feature_columns = ["product_id", "product_name"]
        logger.info(f"Initial feature_columns for products: {feature_columns}")
        logger.info(f"DF columns before feature selection: {df.columns.tolist()}")

        if "demand_profile" in features:
            feature_columns.extend(
                ["avg_sale_amount", "sale_amount_stddev", "active_days"]
            )

        if "stockout_rate" in features:
            feature_columns.extend(["stockout_rate", "avg_stock_level"])

        if "discount_response" in features:
            feature_columns.extend(
                ["avg_discount", "discount_frequency", "discount_response_ratio"]
            )

        if "weather_sensitivity" in features:
            feature_columns.extend(
                [
                    "temperature_sensitivity",
                    "precipitation_sensitivity",
                    "humidity_sensitivity",
                ]
            )

        # Filter columns that exist in the dataframe
        available_columns = [col for col in feature_columns if col in df.columns]
        df = df[available_columns]

        logger.info(f"Selected available columns for products: {available_columns}")
        logger.info(f"DF columns after feature selection: {df.columns.tolist()}")

        # Fill NaN values
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            df[col] = np.where(pd.isnull(df[col]), 0, df[col])

        logger.info(f"Extracted features for {len(df)} products")
        return df

    except Exception as e:
        logger.error(f"Error extracting product features: {e}")
        raise


async def extract_store_features(
    conn: asyncpg.Connection,
    features: List[str],
    start_date: date,
    end_date: date,
    min_data_points: int,
) -> pd.DataFrame:
    """
    Extract store-level features for clustering (real data)
    """
    try:
        query = """
        WITH store_metrics AS (
            SELECT 
                sh.store_id,
                sh.store_name,
                sh.city_id,
                COUNT(DISTINCT s.dt) as active_days,
                COUNT(DISTINCT s.product_id) as unique_products,
                COUNT(*) as total_transactions,
                SUM(CAST(s.sale_amount AS FLOAT)) as total_revenue,
                AVG(CAST(s.sale_amount AS FLOAT)) as avg_transaction_value,
                STDDEV(CAST(s.sale_amount AS FLOAT)) as transaction_value_stddev,
                AVG(CASE WHEN (s.discount > 0) IS TRUE THEN s.discount ELSE NULL END) as avg_discount,
                COUNT(CASE WHEN (s.discount > 0) IS TRUE THEN 1 END)::FLOAT / COUNT(*) as discount_frequency,
                AVG(CASE WHEN s.activity_flag = '1' THEN CAST(s.sale_amount AS FLOAT) ELSE NULL END) as avg_promo_sale,
                AVG(CASE WHEN s.activity_flag = '0' THEN CAST(s.sale_amount AS FLOAT) ELSE NULL END) as avg_regular_sale,
                COUNT(CASE WHEN s.holiday_flag = '1' THEN 1 END)::FLOAT / COUNT(*) as holiday_sales_ratio
            FROM store_hierarchy sh
            JOIN sales_data s ON sh.store_id = s.store_id
            WHERE s.dt BETWEEN $1 AND $2
            GROUP BY sh.store_id, sh.store_name, sh.city_id
            -- Diagnostic: Log counts before filtering
            -- SELECT sh.store_id, COUNT(DISTINCT s.dt) as distinct_active_days FROM store_hierarchy sh JOIN sales_data s ON sh.store_id = s.store_id WHERE s.dt BETWEEN $1 AND $2 GROUP BY sh.store_id
            HAVING COUNT(DISTINCT s.dt) >= $3
        ),
        store_demand_variability AS (
            SELECT 
                store_id,
                STDDEV(daily_revenue) / NULLIF(AVG(daily_revenue), 0) as revenue_cv,
                COUNT(CASE WHEN daily_revenue = 0 THEN 1 END)::FLOAT / COUNT(*) as zero_revenue_ratio
            FROM (
                SELECT 
                    store_id,
                    dt,
                    SUM(CAST(sale_amount AS FLOAT)) as daily_revenue
                FROM sales_data
                WHERE dt BETWEEN $1 AND $2
                GROUP BY store_id, dt
            ) daily_agg
            GROUP BY store_id
        ),
        store_stockout_analysis AS (
            SELECT 
                store_id,
                AVG(CASE WHEN stock_hour6_22_cnt = '0' THEN 1.0 ELSE 0.0 END) as avg_stockout_rate,
                AVG(CAST(stock_hour6_22_cnt AS FLOAT)) as avg_stock_level,
                COUNT(CASE WHEN stock_hour6_22_cnt = '0' THEN 1 END)::FLOAT / COUNT(*) as stockout_frequency
            FROM sales_data
            WHERE dt BETWEEN $1 AND $2
            GROUP BY store_id
        ),
        store_weather_sensitivity AS (
            SELECT 
                s.store_id,
                CORR(s.daily_revenue, w.avg_temperature) as temp_correlation,
                CORR(s.daily_revenue, w.precpt) as precip_correlation,
                CORR(s.daily_revenue, w.avg_humidity) as humidity_correlation
            FROM (
                SELECT 
                    store_id,
                    dt,
                    SUM(CAST(sale_amount AS FLOAT)) as daily_revenue
                FROM sales_data
                WHERE dt BETWEEN $1 AND $2
                GROUP BY store_id, dt
            ) s
            JOIN sales_data w ON s.store_id = w.store_id AND s.dt = w.dt
            GROUP BY s.store_id
            HAVING COUNT(*) >= $3
        )
        SELECT 
            sm.*,
            COALESCE(sdv.revenue_cv, 0) as revenue_coefficient_variation,
            COALESCE(sdv.zero_revenue_ratio, 0) as zero_revenue_ratio,
            COALESCE(ssa.avg_stockout_rate, 0) as avg_stockout_rate,
            COALESCE(ssa.avg_stock_level, 0) as avg_stock_level,
            COALESCE(ssa.stockout_frequency, 0) as stockout_frequency,
            COALESCE(sws.temp_correlation, 0) as temperature_sensitivity,
            COALESCE(sws.precip_correlation, 0) as precipitation_sensitivity,
            COALESCE(sws.humidity_correlation, 0) as humidity_sensitivity,
            CASE 
                WHEN sm.avg_promo_sale IS NOT NULL AND sm.avg_regular_sale IS NOT NULL 
                THEN (sm.avg_promo_sale - sm.avg_regular_sale) / NULLIF(sm.avg_regular_sale, 0)
                ELSE 0 
            END as discount_response_ratio
        FROM store_metrics sm
        LEFT JOIN store_demand_variability sdv ON sm.store_id = sdv.store_id
        LEFT JOIN store_stockout_analysis ssa ON sm.store_id = ssa.store_id
        LEFT JOIN store_weather_sensitivity sws ON sm.store_id = sws.store_id
        ORDER BY sm.store_id
        """
        rows = await conn.fetch(
            query,
            start_date.strftime("%Y-%m-%d"),
            end_date.strftime("%Y-%m-%d"),
            min_data_points,
        )

        if rows:
            logger.debug(f"First row keys from asyncpg: {rows[0].keys()}")

        df = pd.DataFrame(rows, columns=rows[0].keys() if rows else [])
        logger.info(f"Store features query returned {len(df)} rows")

        if df.empty:
            logger.warning("No store features found for clustering query.")
            logger.info("Store DataFrame is empty, returning empty dataframe")
            return pd.DataFrame()

        # Select and prepare features based on request
        feature_columns = ["store_id", "store_name", "city_id"]
        logger.info(f"Initial feature_columns for stores: {feature_columns}")
        logger.info(f"DF columns before feature selection: {df.columns.tolist()}")

        if "demand_profile" in features:
            feature_columns.extend(
                [
                    "total_revenue",
                    "avg_transaction_value",
                    "transaction_value_stddev",
                    "active_days",
                    "unique_products",
                ]
            )

        if "stockout_rate" in features:
            feature_columns.extend(
                ["avg_stockout_rate", "avg_stock_level", "stockout_frequency"]
            )

        if "discount_response" in features:
            feature_columns.extend(
                ["avg_discount", "discount_frequency", "discount_response_ratio"]
            )

        if "weather_sensitivity" in features:
            feature_columns.extend(
                [
                    "temperature_sensitivity",
                    "precipitation_sensitivity",
                    "humidity_sensitivity",
                ]
            )

        # Filter columns that exist in the dataframe
        available_columns = [col for col in feature_columns if col in df.columns]
        df = df[available_columns]

        logger.info(f"Selected available columns for stores: {available_columns}")
        logger.info(f"DF columns after feature selection: {df.columns.tolist()}")

        # Fill NaN values
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            df[col] = np.where(pd.isnull(df[col]), 0, df[col])

        logger.info(f"Extracted features for {len(df)} stores")
        return df

    except Exception as e:
        logger.error(f"Error extracting store features: {e}")
        raise


async def extract_city_features(
    conn: asyncpg.Connection,
    features: List[str],
    start_date: date,
    end_date: date,
    min_data_points: int,
) -> pd.DataFrame:
    """
    Extract city-level features for clustering (real data)
    """
    try:
        query = """
        WITH city_metrics AS (
            SELECT 
                sh.city_id,
                ch.city_name,
                COUNT(DISTINCT sh.store_id) as num_stores,
                COUNT(DISTINCT s.dt) as active_days,
                COUNT(DISTINCT s.product_id) as unique_products,
                COUNT(*) as total_transactions,
                SUM(CAST(s.sale_amount AS FLOAT)) as total_revenue,
                AVG(CAST(s.sale_amount AS FLOAT)) as avg_transaction_value,
                STDDEV(CAST(s.sale_amount AS FLOAT)) as transaction_value_stddev,
                AVG(CASE WHEN (s.discount > 0) IS TRUE THEN s.discount ELSE NULL END) as avg_discount,
                COUNT(CASE WHEN (s.discount > 0) IS TRUE THEN 1 END)::FLOAT / COUNT(*) as discount_frequency,
                AVG(CASE WHEN s.activity_flag = '1' THEN CAST(s.sale_amount AS FLOAT) ELSE NULL END) as avg_promo_sale,
                AVG(CASE WHEN s.activity_flag = '0' THEN CAST(s.sale_amount AS FLOAT) ELSE NULL END) as avg_regular_sale,
                COUNT(CASE WHEN s.holiday_flag = '1' THEN 1 END)::FLOAT / COUNT(*) as holiday_sales_ratio
            FROM store_hierarchy sh
            JOIN sales_data s ON sh.store_id = s.store_id
            JOIN city_hierarchy ch ON sh.city_id = ch.city_id
            WHERE s.dt BETWEEN $1 AND $2
            GROUP BY sh.city_id, ch.city_name
            -- Diagnostic: Log counts before filtering
            -- SELECT sh.city_id, COUNT(DISTINCT s.dt) as distinct_active_days FROM store_hierarchy sh JOIN sales_data s ON sh.store_id = s.store_id WHERE s.dt BETWEEN $1 AND $2 GROUP BY sh.city_id
            HAVING COUNT(DISTINCT s.dt) >= $3
        ),
        city_demand_variability AS (
            SELECT 
                daily_agg.city_id,
                STDDEV(daily_revenue) / NULLIF(AVG(daily_revenue), 0) as revenue_cv,
                COUNT(CASE WHEN daily_revenue = 0 THEN 1 END)::FLOAT / COUNT(*) as zero_revenue_ratio
            FROM (
                SELECT 
                    sh.city_id,
                    s.dt,
                    SUM(CAST(s.sale_amount AS FLOAT)) as daily_revenue
                FROM sales_data s
                JOIN store_hierarchy sh ON s.store_id = sh.store_id
                WHERE s.dt BETWEEN $1 AND $2
                GROUP BY sh.city_id, s.dt
            ) daily_agg
            GROUP BY daily_agg.city_id
        ),
        city_stockout_analysis AS (
            SELECT 
                sh.city_id,
                AVG(CASE WHEN s.stock_hour6_22_cnt = '0' THEN 1.0 ELSE 0.0 END) as avg_stockout_rate,
                AVG(CAST(s.stock_hour6_22_cnt AS FLOAT)) as avg_stock_level,
                COUNT(CASE WHEN s.stock_hour6_22_cnt = '0' THEN 1 END)::FLOAT / COUNT(*) as stockout_frequency
            FROM sales_data s
            JOIN store_hierarchy sh ON s.store_id = sh.store_id
            WHERE s.dt BETWEEN $1 AND $2
            GROUP BY sh.city_id
        ),
        city_weather_data AS (
            SELECT 
                s.city_id,
                AVG(s.avg_temperature) as avg_temperature,
                STDDEV(s.avg_temperature) as temp_variability,
                AVG(s.precpt) as avg_precipitation,
                AVG(s.avg_humidity) as avg_humidity,
                COUNT(CASE WHEN s.precpt > 0 THEN 1 END)::FLOAT / COUNT(*) as rainy_days_ratio
            FROM sales_data s
            WHERE s.dt BETWEEN $1 AND $2
            GROUP BY s.city_id
        ),
        city_weather_sensitivity AS (
            SELECT 
                s.city_id,
                CORR(daily_revenue, w.avg_temperature) as temp_correlation,
                CORR(daily_revenue, w.precpt) as precip_correlation,
                CORR(daily_revenue, w.avg_humidity) as humidity_correlation
            FROM (
                SELECT 
                    sh.city_id,
                    s.dt,
                    SUM(CAST(s.sale_amount AS FLOAT)) as daily_revenue
                FROM sales_data s
                JOIN store_hierarchy sh ON s.store_id = sh.store_id
                WHERE s.dt BETWEEN $1 AND $2
                GROUP BY sh.city_id, s.dt
            ) s
            JOIN sales_data w ON s.city_id = w.city_id AND s.dt = w.dt
            GROUP BY s.city_id
            HAVING COUNT(*) >= $3
        )
        SELECT 
            cm.*,
            COALESCE(cdv.revenue_cv, 0) as revenue_coefficient_variation,
            COALESCE(cdv.zero_revenue_ratio, 0) as zero_revenue_ratio,
            COALESCE(csa.avg_stockout_rate, 0) as avg_stockout_rate,
            COALESCE(csa.avg_stock_level, 0) as avg_stock_level,
            COALESCE(csa.stockout_frequency, 0) as stockout_frequency,
            COALESCE(cwd.avg_temperature, 0) as avg_temperature,
            COALESCE(cwd.temp_variability, 0) as temp_variability,
            COALESCE(cwd.avg_precipitation, 0) as avg_precipitation,
            COALESCE(cwd.avg_humidity, 0) as avg_humidity,
            COALESCE(cwd.rainy_days_ratio, 0) as rainy_days_ratio,
            COALESCE(cws.temp_correlation, 0) as temperature_sensitivity,
            COALESCE(cws.precip_correlation, 0) as precipitation_sensitivity,
            COALESCE(cws.humidity_correlation, 0) as humidity_sensitivity,
            CASE 
                WHEN cm.avg_promo_sale IS NOT NULL AND cm.avg_regular_sale IS NOT NULL 
                THEN (cm.avg_promo_sale - cm.avg_regular_sale) / NULLIF(cm.avg_regular_sale, 0)
                ELSE 0 
            END as discount_response_ratio
        FROM city_metrics cm
        LEFT JOIN city_demand_variability cdv ON cm.city_id = cdv.city_id
        LEFT JOIN city_stockout_analysis csa ON cm.city_id = csa.city_id
        LEFT JOIN city_weather_data cwd ON cm.city_id = cwd.city_id
        LEFT JOIN city_weather_sensitivity cws ON cm.city_id = cws.city_id
        ORDER BY cm.city_id
        """
        rows = await conn.fetch(
            query,
            start_date.strftime("%Y-%m-%d"),
            end_date.strftime("%Y-%m-%d"),
            min_data_points,
        )
        if rows:
            logger.debug(f"First row keys from asyncpg: {rows[0].keys()}")

        df = pd.DataFrame(rows, columns=rows[0].keys() if rows else [])
        logger.info(f"City features query returned {len(df)} rows")
        if df.empty:
            logger.warning("No city features found for clustering query.")
            logger.info("DataFrame is empty, returning empty dataframe")
            return pd.DataFrame()

        # Select and prepare features based on request
        feature_columns = ["city_id", "city_name", "num_stores"]

        if "demand_profile" in features:
            feature_columns.extend(
                [
                    "total_revenue",
                    "avg_transaction_value",
                    "transaction_value_stddev",
                    "active_days",
                    "unique_products",
                ]
            )

        if "stockout_rate" in features:
            feature_columns.extend(
                ["avg_stockout_rate", "avg_stock_level", "stockout_frequency"]
            )

        if "discount_response" in features:
            feature_columns.extend(
                ["avg_discount", "discount_frequency", "discount_response_ratio"]
            )

        if "weather_sensitivity" in features:
            feature_columns.extend(
                [
                    "avg_temperature",
                    "temp_variability",
                    "avg_precipitation",
                    "avg_humidity",
                    "rainy_days_ratio",
                    "temperature_sensitivity",
                    "precipitation_sensitivity",
                    "humidity_sensitivity",
                ]
            )

        # Filter columns that exist in the dataframe
        available_columns = [col for col in feature_columns if col in df.columns]
        df = df[available_columns]

        # Fill NaN values
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            df[col] = np.where(pd.isnull(df[col]), 0, df[col])

        logger.info(f"Extracted features for {len(df)} cities")
        return df

    except Exception as e:
        logger.error(f"Error extracting city features: {e}")
        raise


async def perform_clustering(
    features_df: pd.DataFrame, algorithm: str, n_clusters: Optional[int] = None
) -> Dict[str, Any]:
    """
    Perform clustering analysis using specified algorithm
    """
    try:
        # Prepare features for clustering
        entity_columns = []
        if "product_id" in features_df.columns:
            entity_columns = ["product_id", "product_name"]
        elif "store_id" in features_df.columns:
            entity_columns = ["store_id", "store_name", "city_id"]
        elif "city_id" in features_df.columns:
            entity_columns = ["city_id", "num_stores"]

        # Get numeric features for clustering
        numeric_features = features_df.select_dtypes(
            include=[np.number]
        ).columns.tolist()

        # Remove entity identifier columns from numeric features
        numeric_features = [
            col for col in numeric_features if col not in entity_columns
        ]

        if not numeric_features:
            raise ValueError("No numeric features available for clustering")

        X = features_df[numeric_features].values

        # Handle infinite values and NaN
        if not isinstance(X, np.ndarray):
            X = np.array(X)
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

        # Standardize features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Perform clustering
        if algorithm.lower() == "kmeans":
            # Determine optimal number of clusters if not specified
            if n_clusters is None:
                n_clusters = determine_optimal_clusters(X_scaled)

            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init="auto")
            cluster_labels = clusterer.fit_predict(X_scaled)

        elif algorithm.lower() == "dbscan":
            # Use DBSCAN with automatic parameter selection
            eps, min_samples = determine_dbscan_parameters(X_scaled)
            clusterer = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = clusterer.fit_predict(X_scaled)
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)

        else:
            raise ValueError(f"Unsupported clustering algorithm: {algorithm}")

        # Add cluster labels to dataframe
        features_df_clustered = features_df.copy()
        features_df_clustered["cluster_id"] = cluster_labels

        return {
            "algorithm": algorithm,
            "n_clusters": n_clusters,
            "cluster_labels": cluster_labels,
            "features_df": features_df_clustered,
            "numeric_features": numeric_features,
            "scaler": scaler,
            "X_scaled": X_scaled,
            "clusterer": clusterer,
        }

    except Exception as e:
        logger.error(f"Error performing clustering: {e}")
        raise


def determine_optimal_clusters(X_scaled: np.ndarray, max_clusters: int = 10) -> int:
    """
    Determine optimal number of clusters using elbow method and silhouette score
    """
    try:
        n_samples = X_scaled.shape[0]
        max_clusters = min(max_clusters, n_samples - 1)

        if max_clusters < 2:
            return 2

        silhouette_scores = []
        inertias = []

        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init="auto")
            cluster_labels = kmeans.fit_predict(X_scaled)

            if len(set(cluster_labels)) > 1:  # Ensure we have multiple clusters
                silhouette_avg = silhouette_score(X_scaled, cluster_labels)
                silhouette_scores.append(silhouette_avg)
                inertias.append(kmeans.inertia_)
            else:
                silhouette_scores.append(0)
                inertias.append(float("inf"))

        # Find optimal k using silhouette score
        if silhouette_scores:
            optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2
        else:
            optimal_k = 3

        return min(optimal_k, max_clusters)

    except Exception as e:
        logger.error(f"Error determining optimal clusters: {e}")
        return 3


def determine_dbscan_parameters(X_scaled: np.ndarray) -> Tuple[float, int]:
    """
    Determine optimal DBSCAN parameters
    """
    try:
        from sklearn.neighbors import NearestNeighbors

        # Calculate k-distance for determining eps
        k = min(4, X_scaled.shape[0] - 1)
        neighbors = NearestNeighbors(n_neighbors=k)
        neighbors_fit = neighbors.fit(X_scaled)
        distances, indices = neighbors_fit.kneighbors(X_scaled)

        # Sort distances and find elbow point
        distances = np.sort(distances[:, k - 1], axis=0)

        # Use 75th percentile as eps
        eps = np.percentile(distances, 75)

        # Set min_samples based on dimensionality
        min_samples = max(3, X_scaled.shape[1] + 1)

        return float(eps), int(min_samples)

    except Exception as e:
        logger.error(f"Error determining DBSCAN parameters: {e}")
        return 0.5, 5


async def generate_cluster_profiles(
    conn: asyncpg.Connection,
    clustering_result: Dict[str, Any],
    entity_type: str,
    features: List[str],
) -> List[ClusterProfile]:
    """
    Generate detailed profiles for each cluster
    """
    try:
        features_df = clustering_result["features_df"]
        numeric_features = clustering_result["numeric_features"]
        n_clusters = clustering_result["n_clusters"]

        cluster_profiles = []

        for cluster_id in range(n_clusters):
            cluster_data = features_df[features_df["cluster_id"] == cluster_id]

            if cluster_data.empty:
                continue

            # Calculate cluster characteristics
            characteristics = {}
            key_metrics = {}

            for feature in numeric_features:
                if feature in cluster_data.columns:
                    characteristics[feature] = {
                        "mean": float(cluster_data[feature].mean()),
                        "std": float(cluster_data[feature].std()),
                        "min": float(cluster_data[feature].min()),
                        "max": float(cluster_data[feature].max()),
                    }
                    key_metrics[f"{feature}_mean"] = float(cluster_data[feature].mean())

            # Generate cluster name based on key characteristics
            cluster_name = generate_cluster_name(
                cluster_id, characteristics, entity_type
            )

            # Get representative entities
            representative_entities = get_representative_entities(
                cluster_data, entity_type
            )

            cluster_profile = ClusterProfile(
                cluster_id=cluster_id,
                cluster_name=cluster_name,
                entity_count=len(cluster_data),
                characteristics=characteristics,
                key_metrics=key_metrics,
                representative_entities=representative_entities,
            )

            cluster_profiles.append(cluster_profile)

        return cluster_profiles

    except Exception as e:
        logger.error(f"Error generating cluster profiles: {e}")
        raise


def generate_cluster_name(
    cluster_id: int, characteristics: Dict[str, Any], entity_type: str
) -> str:
    """
    Generate descriptive name for cluster based on characteristics
    """
    try:
        # Define key metrics for naming
        key_metrics = {}

        if entity_type == "products":
            if "avg_sale_amount" in characteristics:
                key_metrics["revenue"] = characteristics["avg_sale_amount"]["mean"]
            if "stockout_rate" in characteristics:
                key_metrics["stockout"] = characteristics["stockout_rate"]["mean"]
            if "discount_frequency" in characteristics:
                key_metrics["discount"] = characteristics["discount_frequency"]["mean"]

        elif entity_type == "stores":
            if "total_revenue" in characteristics:
                key_metrics["revenue"] = characteristics["total_revenue"]["mean"]
            if "avg_stockout_rate" in characteristics:
                key_metrics["stockout"] = characteristics["avg_stockout_rate"]["mean"]
            if "discount_frequency" in characteristics:
                key_metrics["discount"] = characteristics["discount_frequency"]["mean"]

        elif entity_type == "cities":
            if "total_revenue" in characteristics:
                key_metrics["revenue"] = characteristics["total_revenue"]["mean"]
            if "num_stores" in characteristics:
                key_metrics["stores"] = characteristics["num_stores"]["mean"]
            if "avg_stockout_rate" in characteristics:
                key_metrics["stockout"] = characteristics["avg_stockout_rate"]["mean"]

        # Generate name based on key metrics
        name_parts = []

        if "revenue" in key_metrics:
            if key_metrics["revenue"] > 100:
                name_parts.append("High-Revenue")
            elif key_metrics["revenue"] > 50:
                name_parts.append("Medium-Revenue")
            else:
                name_parts.append("Low-Revenue")

        if "stockout" in key_metrics:
            if key_metrics["stockout"] > 0.2:
                name_parts.append("High-Stockout")
            elif key_metrics["stockout"] > 0.1:
                name_parts.append("Medium-Stockout")
            else:
                name_parts.append("Low-Stockout")

        if "discount" in key_metrics:
            if key_metrics["discount"] > 0.3:
                name_parts.append("Discount-Sensitive")
            elif key_metrics["discount"] > 0.1:
                name_parts.append("Moderately-Discount-Sensitive")

        if "stores" in key_metrics:
            if key_metrics["stores"] > 10:
                name_parts.append("Multi-Store")
            elif key_metrics["stores"] > 5:
                name_parts.append("Medium-Store")
            else:
                name_parts.append("Few-Store")

        if name_parts:
            return f"Cluster {cluster_id}: {' '.join(name_parts)}"
        else:
            return f"Cluster {cluster_id}: Standard"

    except Exception as e:
        logger.error(f"Error generating cluster name: {e}")
        return f"Cluster {cluster_id}"


def get_representative_entities(
    cluster_data: pd.DataFrame, entity_type: str
) -> List[Dict[str, Any]]:
    """
    Get representative entities from cluster
    """
    try:
        representatives = []

        # Get up to 5 representative entities
        sample_size = min(5, len(cluster_data))
        sample_data = cluster_data.sample(n=sample_size, random_state=42)

        for _, row in sample_data.iterrows():
            if entity_type == "products":
                representatives.append(
                    {
                        "id": str(row["product_id"]),
                        "name": row.get("product_name", f"Product {row['product_id']}"),
                        "key_metrics": {
                            "avg_sale_amount": row.get("avg_sale_amount", 0),
                            "stockout_rate": row.get("stockout_rate", 0),
                            "discount_frequency": row.get("discount_frequency", 0),
                        },
                    }
                )
            elif entity_type == "stores":
                representatives.append(
                    {
                        "id": str(row["store_id"]),
                        "name": row.get("store_name", f"Store {row['store_id']}"),
                        "city_id": str(row.get("city_id", "")),
                        "key_metrics": {
                            "total_revenue": row.get("total_revenue", 0),
                            "avg_stockout_rate": row.get("avg_stockout_rate", 0),
                            "discount_frequency": row.get("discount_frequency", 0),
                        },
                    }
                )
            elif entity_type == "cities":
                representatives.append(
                    {
                        "id": str(row["city_id"]),
                        "name": f"City {row['city_id']}",
                        "key_metrics": {
                            "total_revenue": row.get("total_revenue", 0),
                            "num_stores": row.get("num_stores", 0),
                            "avg_stockout_rate": row.get("avg_stockout_rate", 0),
                        },
                    }
                )

        return representatives

    except Exception as e:
        logger.error(f"Error getting representative entities: {e}")
        return []


async def generate_entity_assignments(
    conn: asyncpg.Connection, clustering_result: Dict[str, Any], entity_type: str
) -> List[EntityClusterInfo]:
    """
    Generate entity assignments with cluster information
    """
    try:
        features_df = clustering_result["features_df"]
        X_scaled = clustering_result["X_scaled"]
        clusterer = clustering_result["clusterer"]

        entity_assignments = []

        for idx, row in features_df.iterrows():
            cluster_id = row["cluster_id"]

            # Debugging: Check the content of the row being processed
            logger.debug(f"Processing entity row: {row.to_dict()}")

            # Calculate similarity score (distance to cluster center)
            if hasattr(clusterer, "cluster_centers_"):
                # For KMeans
                cluster_center = clusterer.cluster_centers_[cluster_id]
                entity_features = X_scaled[idx]
                similarity_score = 1.0 / (
                    1.0 + np.linalg.norm(entity_features - cluster_center)
                )
            else:
                # For DBSCAN or other algorithms
                similarity_score = 0.8  # Default similarity

            if entity_type == "products":
                entity_info = EntityClusterInfo(
                    entity_id=str(row["product_id"]),
                    entity_name=row.get("product_name", f"Product {row['product_id']}"),
                    cluster_id=cluster_id,
                    cluster_name=f"Cluster {cluster_id}",
                    similarity_score=float(similarity_score),
                    key_attributes={
                        "avg_sale_amount": row.get("avg_sale_amount", 0),
                        "stockout_rate": row.get("stockout_rate", 0),
                        "discount_frequency": row.get("discount_frequency", 0),
                        "temperature_sensitivity": row.get(
                            "temperature_sensitivity", 0
                        ),
                    },
                )
            elif entity_type == "stores":
                entity_info = EntityClusterInfo(
                    entity_id=str(row["store_id"]),
                    entity_name=row.get("store_name", f"Store {row['store_id']}"),
                    cluster_id=cluster_id,
                    cluster_name=f"Cluster {cluster_id}",
                    similarity_score=float(similarity_score),
                    key_attributes={
                        "city_id": str(row.get("city_id", "")),
                        "total_revenue": row.get("total_revenue", 0),
                        "avg_stockout_rate": row.get("avg_stockout_rate", 0),
                        "discount_frequency": row.get("discount_frequency", 0),
                    },
                )
            elif entity_type == "cities":
                entity_info = EntityClusterInfo(
                    entity_id=str(row["city_id"]),
                    entity_name=row.get("city_name", f"City {row['city_id']}"),
                    cluster_id=cluster_id,
                    cluster_name=f"Cluster {cluster_id}",
                    similarity_score=float(similarity_score),
                    key_attributes={
                        "num_stores": row.get("num_stores", 0),
                        "total_revenue": row.get("total_revenue", 0),
                        "avg_stockout_rate": row.get("avg_stockout_rate", 0),
                        "avg_temperature": row.get("avg_temperature", 0),
                    },
                )

            entity_assignments.append(entity_info)

        return entity_assignments

    except Exception as e:
        logger.error(f"Error generating entity assignments: {e}")
        raise


def calculate_cluster_quality_metrics(
    clustering_result: Dict[str, Any]
) -> Dict[str, float]:
    """
    Calculate clustering quality metrics
    """
    try:
        X_scaled = clustering_result["X_scaled"]
        cluster_labels = clustering_result["cluster_labels"]

        quality_metrics = {}

        # Calculate silhouette score
        if len(set(cluster_labels)) > 1:
            silhouette_avg = silhouette_score(X_scaled, cluster_labels)
            quality_metrics["silhouette_score"] = float(silhouette_avg)

            # Calculate Calinski-Harabasz score
            ch_score = calinski_harabasz_score(X_scaled, cluster_labels)
            quality_metrics["calinski_harabasz_score"] = float(ch_score)
        else:
            quality_metrics["silhouette_score"] = 0.0
            quality_metrics["calinski_harabasz_score"] = 0.0

        # Calculate inertia for KMeans
        if hasattr(clustering_result["clusterer"], "inertia_"):
            quality_metrics["inertia"] = float(clustering_result["clusterer"].inertia_)

        # Calculate cluster size statistics
        unique_labels, counts = np.unique(cluster_labels, return_counts=True)
        quality_metrics["avg_cluster_size"] = float(np.mean(counts))
        quality_metrics["cluster_size_std"] = float(np.std(counts))
        quality_metrics["min_cluster_size"] = float(np.min(counts))
        quality_metrics["max_cluster_size"] = float(np.max(counts))

        return quality_metrics

    except Exception as e:
        logger.error(f"Error calculating cluster quality metrics: {e}")
        return {}


def generate_clustering_insights(
    cluster_profiles: List[ClusterProfile], entity_type: str
) -> List[str]:
    """
    Generate insights from clustering analysis
    """
    try:
        insights = []

        if not cluster_profiles:
            return ["No clusters found in the data."]

        # Overall insights
        total_entities = sum(profile.entity_count for profile in cluster_profiles)
        insights.append(
            f"Identified {len(cluster_profiles)} distinct {entity_type} clusters from {total_entities} entities."
        )

        # Cluster size insights
        largest_cluster = max(cluster_profiles, key=lambda x: x.entity_count)
        smallest_cluster = min(cluster_profiles, key=lambda x: x.entity_count)

        insights.append(
            f"Largest cluster: '{largest_cluster.cluster_name}' with {largest_cluster.entity_count} entities."
        )
        insights.append(
            f"Smallest cluster: '{smallest_cluster.cluster_name}' with {smallest_cluster.entity_count} entities."
        )

        # Feature-specific insights
        if entity_type == "products":
            # Revenue insights
            revenue_clusters = [
                p for p in cluster_profiles if "avg_sale_amount_mean" in p.key_metrics
            ]
            if revenue_clusters:
                high_revenue = max(
                    revenue_clusters,
                    key=lambda x: x.key_metrics["avg_sale_amount_mean"],
                )
                insights.append(
                    f"Highest revenue cluster: '{high_revenue.cluster_name}' with average sale amount of ${high_revenue.key_metrics['avg_sale_amount_mean']:.2f}."
                )

            # Stockout insights
            stockout_clusters = [
                p for p in cluster_profiles if "stockout_rate_mean" in p.key_metrics
            ]
            if stockout_clusters:
                high_stockout = max(
                    stockout_clusters, key=lambda x: x.key_metrics["stockout_rate_mean"]
                )
                insights.append(
                    f"Highest stockout risk cluster: '{high_stockout.cluster_name}' with {high_stockout.key_metrics['stockout_rate_mean']:.1%} stockout rate."
                )

        elif entity_type == "stores":
            # Store performance insights
            revenue_clusters = [
                p for p in cluster_profiles if "total_revenue_mean" in p.key_metrics
            ]
            if revenue_clusters:
                high_revenue = max(
                    revenue_clusters, key=lambda x: x.key_metrics["total_revenue_mean"]
                )
                insights.append(
                    f"Top performing store cluster: '{high_revenue.cluster_name}' with average revenue of ${high_revenue.key_metrics['total_revenue_mean']:,.2f}."
                )

        elif entity_type == "cities":
            # City market insights
            store_clusters = [
                p for p in cluster_profiles if "num_stores_mean" in p.key_metrics
            ]
            if store_clusters:
                multi_store = max(
                    store_clusters, key=lambda x: x.key_metrics["num_stores_mean"]
                )
                insights.append(
                    f"Largest market cluster: '{multi_store.cluster_name}' with average of {multi_store.key_metrics['num_stores_mean']:.1f} stores per city."
                )

        # Actionable insights
        insights.append(
            "These clusters can be used for targeted strategies, resource allocation, and performance optimization."
        )

        return insights

    except Exception as e:
        logger.error(f"Error generating clustering insights: {e}")
        return ["Error generating insights from clustering analysis."]


def generate_clustering_recommendations(
    cluster_profiles: List[ClusterProfile], entity_type: str
) -> List[Dict[str, Any]]:
    """
    Generate actionable recommendations based on clustering analysis
    """
    try:
        recommendations = []

        if not cluster_profiles:
            return recommendations

        for profile in cluster_profiles:
            if entity_type == "products":
                # Product-specific recommendations
                if "stockout_rate_mean" in profile.key_metrics:
                    if profile.key_metrics["stockout_rate_mean"] > 0.2:
                        recommendations.append(
                            {
                                "cluster_id": profile.cluster_id,
                                "cluster_name": profile.cluster_name,
                                "recommendation_type": "inventory_management",
                                "priority": "high",
                                "action": "Increase safety stock levels",
                                "reason": f'High stockout rate of {profile.key_metrics["stockout_rate_mean"]:.1%}',
                                "expected_impact": "Reduce stockouts by 30-50%",
                            }
                        )

                if "discount_frequency_mean" in profile.key_metrics:
                    if profile.key_metrics["discount_frequency_mean"] > 0.3:
                        recommendations.append(
                            {
                                "cluster_id": profile.cluster_id,
                                "cluster_name": profile.cluster_name,
                                "recommendation_type": "pricing_strategy",
                                "priority": "medium",
                                "action": "Implement dynamic pricing",
                                "reason": f'High discount sensitivity ({profile.key_metrics["discount_frequency_mean"]:.1%})',
                                "expected_impact": "Optimize profit margins by 10-15%",
                            }
                        )

            elif entity_type == "stores":
                # Store-specific recommendations
                if "avg_stockout_rate_mean" in profile.key_metrics:
                    if profile.key_metrics["avg_stockout_rate_mean"] > 0.15:
                        recommendations.append(
                            {
                                "cluster_id": profile.cluster_id,
                                "cluster_name": profile.cluster_name,
                                "recommendation_type": "supply_chain",
                                "priority": "high",
                                "action": "Implement automated reordering",
                                "reason": f'High stockout rate of {profile.key_metrics["avg_stockout_rate_mean"]:.1%}',
                                "expected_impact": "Improve stock availability by 25-40%",
                            }
                        )

                if "total_revenue_mean" in profile.key_metrics:
                    if profile.key_metrics["total_revenue_mean"] < 50000:
                        recommendations.append(
                            {
                                "cluster_id": profile.cluster_id,
                                "cluster_name": profile.cluster_name,
                                "recommendation_type": "performance_improvement",
                                "priority": "medium",
                                "action": "Focus on customer acquisition",
                                "reason": f'Low revenue performance (${profile.key_metrics["total_revenue_mean"]:,.2f})',
                                "expected_impact": "Increase revenue by 20-30%",
                            }
                        )

            elif entity_type == "cities":
                # City-specific recommendations
                if "num_stores_mean" in profile.key_metrics:
                    if profile.key_metrics["num_stores_mean"] < 3:
                        recommendations.append(
                            {
                                "cluster_id": profile.cluster_id,
                                "cluster_name": profile.cluster_name,
                                "recommendation_type": "market_expansion",
                                "priority": "medium",
                                "action": "Consider market expansion",
                                "reason": f'Low store density ({profile.key_metrics["num_stores_mean"]:.1f} stores)',
                                "expected_impact": "Increase market coverage by 50-100%",
                            }
                        )

                if "avg_stockout_rate_mean" in profile.key_metrics:
                    if profile.key_metrics["avg_stockout_rate_mean"] > 0.2:
                        recommendations.append(
                            {
                                "cluster_id": profile.cluster_id,
                                "cluster_name": profile.cluster_name,
                                "recommendation_type": "regional_strategy",
                                "priority": "high",
                                "action": "Implement regional distribution center",
                                "reason": f'High regional stockout rate ({profile.key_metrics["avg_stockout_rate_mean"]:.1%})',
                                "expected_impact": "Reduce regional stockouts by 40-60%",
                            }
                        )

        return recommendations

    except Exception as e:
        logger.error(f"Error generating clustering recommendations: {e}")
        return []


@router.get("/clustering-features")
async def get_available_clustering_features():
    """
    Get available features for clustering analysis
    """
    try:
        features = {
            "products": {
                "demand_profile": [
                    "avg_sale_amount",
                    "sale_amount_stddev",
                    "active_days",
                ],
                "stockout_rate": ["stockout_rate", "avg_stock_level"],
                "discount_response": [
                    "avg_discount",
                    "discount_frequency",
                    "discount_response_ratio",
                ],
                "weather_sensitivity": [
                    "temperature_sensitivity",
                    "precipitation_sensitivity",
                    "humidity_sensitivity",
                ],
            },
            "stores": {
                "demand_profile": [
                    "total_revenue",
                    "avg_transaction_value",
                    "transaction_value_stddev",
                    "active_days",
                    "unique_products",
                ],
                "stockout_rate": [
                    "avg_stockout_rate",
                    "avg_stock_level",
                    "stockout_frequency",
                ],
                "discount_response": [
                    "avg_discount",
                    "discount_frequency",
                    "discount_response_ratio",
                ],
                "weather_sensitivity": [
                    "temperature_sensitivity",
                    "precipitation_sensitivity",
                    "humidity_sensitivity",
                ],
            },
            "cities": {
                "demand_profile": [
                    "total_revenue",
                    "avg_transaction_value",
                    "transaction_value_stddev",
                    "active_days",
                    "unique_products",
                ],
                "stockout_rate": [
                    "avg_stockout_rate",
                    "avg_stock_level",
                    "stockout_frequency",
                ],
                "discount_response": [
                    "avg_discount",
                    "discount_frequency",
                    "discount_response_ratio",
                ],
                "weather_sensitivity": [
                    "avg_temperature",
                    "temp_variability",
                    "avg_precipitation",
                    "avg_humidity",
                    "rainy_days_ratio",
                    "temperature_sensitivity",
                    "precipitation_sensitivity",
                    "humidity_sensitivity",
                ],
            },
        }

        return {
            "success": True,
            "features": features,
            "algorithms": ["kmeans", "dbscan"],
            "description": "Available features and algorithms for clustering analysis",
        }

    except Exception as e:
        logger.error(f"Error getting clustering features: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/cluster-comparison")
async def compare_clusters(request: dict):
    """
    Compare different clustering approaches
    """
    try:
        await db_manager.initialize()

        entity_type = request.get("entity_type", "products")
        features = request.get("features", ["demand_profile", "stockout_rate"])
        analysis_period_days = request.get("analysis_period_days", 365)
        min_data_points = request.get("min_data_points", 30)

        logger.info(f"Comparing clustering approaches for {entity_type}")

        # Get connection
        async with db_manager.get_connection() as conn:
            # Extract features
            features_df = await extract_clustering_features(
                conn, entity_type, features, analysis_period_days, min_data_points
            )

            if features_df.empty:
                raise HTTPException(
                    status_code=400,
                    detail="Insufficient data for clustering comparison",
                )

            # Compare different algorithms and cluster numbers
            comparison_results = []

            # KMeans with different cluster numbers
            for n_clusters in [2, 3, 4, 5]:
                if n_clusters < len(features_df):
                    kmeans_result = await perform_clustering(
                        features_df, "kmeans", n_clusters
                    )
                    quality_metrics = calculate_cluster_quality_metrics(kmeans_result)

                    comparison_results.append(
                        {
                            "algorithm": "kmeans",
                            "n_clusters": n_clusters,
                            "quality_metrics": quality_metrics,
                            "silhouette_score": quality_metrics.get(
                                "silhouette_score", 0
                            ),
                        }
                    )

            # DBSCAN
            dbscan_result = await perform_clustering(features_df, "dbscan", None)
            quality_metrics = calculate_cluster_quality_metrics(dbscan_result)

            comparison_results.append(
                {
                    "algorithm": "dbscan",
                    "n_clusters": dbscan_result["n_clusters"],
                    "quality_metrics": quality_metrics,
                    "silhouette_score": quality_metrics.get("silhouette_score", 0),
                }
            )

            # Find best approach
            best_approach = max(comparison_results, key=lambda x: x["silhouette_score"])

            return {
                "success": True,
                "entity_type": entity_type,
                "comparison_results": comparison_results,
                "best_approach": best_approach,
                "recommendation": f"Use {best_approach['algorithm']} with {best_approach['n_clusters']} clusters for optimal results",
            }

    except Exception as e:
        logger.error(f"Error comparing clusters: {e}")
        raise HTTPException(status_code=500, detail=str(e))
